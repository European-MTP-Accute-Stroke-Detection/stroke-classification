{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Eunmi Joo\n",
    "\n",
    "Patient-wise Stroke Type Classifier Model Implementation - Transformer Architecture\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(device)\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import logging\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import pydicom\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/ceph/inestp02/stroke_classifier/data/patientwise/preprocess/features_resnet152.pickle\", \"rb\") as f:\n",
    "    features = pickle.load(f)\n",
    "with open('/ceph/inestp02/stroke_classifier/data/patientwise/preprocess/patient_data_combine_wo_dicom.pickle', 'rb') as f:\n",
    "    patientwise_scans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imglen(data):\n",
    "    img_len =[]\n",
    "    for key in data:\n",
    "        img_len.append(len(data[key]['img_preprocess']))\n",
    "    return img_len\n",
    "img_len = extract_imglen(patientwise_scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientwiseDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 80):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(CNNTransformer, self).__init__()\n",
    "        \n",
    "        # fine_cnn = cnn_model\n",
    "        # modules = list(fine_cnn.children())[:-1]\n",
    "        # self.cnn = nn.Sequential(*modules)\n",
    "        \n",
    "        # self.embedding = nn.Linear(in_features, d_model)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(input_dim, nhead, dim_feedforward, dropout, activation)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        x.shape\n",
    "        \n",
    "        x = x.mean(dim=0)  # Average pooling across the sequence length dimension\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "    \n",
    "# Set the hyperparameters\n",
    "\n",
    "\n",
    "model = models.resnet152(weights=\"DEFAULT\")\n",
    "# in_features = model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_path = f'models_state_dict/runs/'\n",
    "if not os.path.exists(state_dict_path):\n",
    "        os.makedirs(state_dict_path)\n",
    "\n",
    "#model save path\n",
    "model_path = f'models/runs/'\n",
    "if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "#tensorboard path\n",
    "tensorboard_path = f'tensorboard/runs/'\n",
    "if not os.path.exists(tensorboard_path):\n",
    "        os.makedirs(tensorboard_path)\n",
    "\n",
    "\n",
    "#writing log for tensorboard\n",
    "writer = SummaryWriter(log_dir = f'{tensorboard_path}/resnet152')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = torch.Tensor(list(i['patient_label'] for i in patientwise_scans.values()))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "train_dataset = PatientwiseDataset(X_train, y_train)\n",
    "test_dataset = PatientwiseDataset(X_test, y_test)\n",
    "val_dataset = PatientwiseDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batched training\n",
    "num_epochs = 500\n",
    "random_state = 42\n",
    "maxlen = 80\n",
    "input_dim = 2048\n",
    "output_dim = 3\n",
    "d_model = 2048\n",
    "dropout = 0.05\n",
    "batch_size = 8\n",
    "\n",
    "best_model = None\n",
    "best_config = None\n",
    "best_val_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lst_dim_feedforward = [128, 256, 512, 1024, 2048]\n",
    "lst_nhead = [2, 4, 8]\n",
    "lst_learning_rate = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "lst_weight_decay = [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "\n",
    "config_list = [lst_dim_feedforward, lst_nhead, lst_learning_rate, lst_weight_decay]\n",
    "configs = list(itertools.product(*config_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98776f25e04b46c88cd42f02eb30a9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.0830, Train Accuracy: 0.6897\n",
      "Epoch [1/500], Validation Loss: 0.0516, Validation Accuracy: 0.8214\n",
      "Epoch [2/500], Train Loss: 0.0462, Train Accuracy: 0.8336\n",
      "Epoch [2/500], Validation Loss: 0.0354, Validation Accuracy: 0.8690\n",
      "Epoch [3/500], Train Loss: 0.0366, Train Accuracy: 0.8668\n",
      "Epoch [3/500], Validation Loss: 0.0461, Validation Accuracy: 0.8095\n",
      "Epoch [4/500], Train Loss: 0.0292, Train Accuracy: 0.9001\n",
      "Epoch [4/500], Validation Loss: 0.0555, Validation Accuracy: 0.8214\n",
      "Epoch [5/500], Train Loss: 0.0200, Train Accuracy: 0.9361\n",
      "Epoch [5/500], Validation Loss: 0.0431, Validation Accuracy: 0.8452\n",
      "Epoch [6/500], Train Loss: 0.0140, Train Accuracy: 0.9561\n",
      "Epoch [6/500], Validation Loss: 0.0598, Validation Accuracy: 0.8571\n",
      "Epoch [7/500], Train Loss: 0.0094, Train Accuracy: 0.9654\n",
      "Epoch [7/500], Validation Loss: 0.0751, Validation Accuracy: 0.8095\n",
      "Epoch [8/500], Train Loss: 0.0057, Train Accuracy: 0.9893\n",
      "Epoch [8/500], Validation Loss: 0.1082, Validation Accuracy: 0.7619\n",
      "Epoch [9/500], Train Loss: 0.0019, Train Accuracy: 0.9960\n",
      "Epoch [9/500], Validation Loss: 0.1448, Validation Accuracy: 0.7143\n",
      "Epoch [10/500], Train Loss: 0.0020, Train Accuracy: 0.9947\n",
      "Epoch [10/500], Validation Loss: 0.1549, Validation Accuracy: 0.7738\n",
      "Epoch [11/500], Train Loss: 0.0057, Train Accuracy: 0.9800\n",
      "Epoch [11/500], Validation Loss: 0.1533, Validation Accuracy: 0.7738\n",
      "Epoch [12/500], Train Loss: 0.0119, Train Accuracy: 0.9667\n",
      "Epoch [12/500], Validation Loss: 0.1324, Validation Accuracy: 0.7857\n",
      "Epoch [13/500], Train Loss: 0.0020, Train Accuracy: 0.9947\n",
      "Epoch [13/500], Validation Loss: 0.1133, Validation Accuracy: 0.8690\n",
      "Epoch [14/500], Train Loss: 0.0002, Train Accuracy: 1.0000\n",
      "Epoch [14/500], Validation Loss: 0.0984, Validation Accuracy: 0.7976\n",
      "Epoch [15/500], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
      "Epoch [15/500], Validation Loss: 0.1054, Validation Accuracy: 0.8571\n",
      "Epoch [16/500], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
      "Epoch [16/500], Validation Loss: 0.1070, Validation Accuracy: 0.8214\n",
      "Epoch [17/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [17/500], Validation Loss: 0.1095, Validation Accuracy: 0.8333\n",
      "Epoch [18/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [18/500], Validation Loss: 0.1121, Validation Accuracy: 0.8333\n",
      "Epoch [19/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [19/500], Validation Loss: 0.1138, Validation Accuracy: 0.8333\n",
      "Epoch [20/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [20/500], Validation Loss: 0.1159, Validation Accuracy: 0.8333\n",
      "Epoch [21/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [21/500], Validation Loss: 0.1178, Validation Accuracy: 0.8333\n",
      "Epoch [22/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [22/500], Validation Loss: 0.1187, Validation Accuracy: 0.8214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175c407697f8457ca08bf44cf84df684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.0848, Train Accuracy: 0.6991\n",
      "Epoch [1/500], Validation Loss: 0.0497, Validation Accuracy: 0.8333\n",
      "Epoch [2/500], Train Loss: 0.0427, Train Accuracy: 0.8429\n",
      "Epoch [2/500], Validation Loss: 0.0407, Validation Accuracy: 0.8571\n",
      "Epoch [3/500], Train Loss: 0.0366, Train Accuracy: 0.8735\n",
      "Epoch [3/500], Validation Loss: 0.0503, Validation Accuracy: 0.8571\n",
      "Epoch [4/500], Train Loss: 0.0262, Train Accuracy: 0.9161\n",
      "Epoch [4/500], Validation Loss: 0.0365, Validation Accuracy: 0.8571\n",
      "Epoch [5/500], Train Loss: 0.0264, Train Accuracy: 0.9095\n",
      "Epoch [5/500], Validation Loss: 0.0489, Validation Accuracy: 0.8571\n",
      "Epoch [6/500], Train Loss: 0.0102, Train Accuracy: 0.9694\n",
      "Epoch [6/500], Validation Loss: 0.0711, Validation Accuracy: 0.8690\n",
      "Epoch [7/500], Train Loss: 0.0072, Train Accuracy: 0.9774\n",
      "Epoch [7/500], Validation Loss: 0.0827, Validation Accuracy: 0.8333\n",
      "Epoch [8/500], Train Loss: 0.0024, Train Accuracy: 0.9947\n",
      "Epoch [8/500], Validation Loss: 0.0757, Validation Accuracy: 0.8690\n",
      "Epoch [9/500], Train Loss: 0.0009, Train Accuracy: 0.9987\n",
      "Epoch [9/500], Validation Loss: 0.0939, Validation Accuracy: 0.8452\n",
      "Epoch [10/500], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
      "Epoch [10/500], Validation Loss: 0.1020, Validation Accuracy: 0.8571\n",
      "Epoch [11/500], Train Loss: 0.0001, Train Accuracy: 1.0000\n",
      "Epoch [11/500], Validation Loss: 0.1077, Validation Accuracy: 0.8690\n",
      "Epoch [12/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [12/500], Validation Loss: 0.1108, Validation Accuracy: 0.8690\n",
      "Epoch [13/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [13/500], Validation Loss: 0.1117, Validation Accuracy: 0.8571\n",
      "Epoch [14/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [14/500], Validation Loss: 0.1156, Validation Accuracy: 0.8690\n",
      "Epoch [15/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [15/500], Validation Loss: 0.1164, Validation Accuracy: 0.8690\n",
      "Epoch [16/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [16/500], Validation Loss: 0.1184, Validation Accuracy: 0.8690\n",
      "Epoch [17/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [17/500], Validation Loss: 0.1200, Validation Accuracy: 0.8690\n",
      "Epoch [18/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [18/500], Validation Loss: 0.1223, Validation Accuracy: 0.8690\n",
      "Epoch [19/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [19/500], Validation Loss: 0.1225, Validation Accuracy: 0.8690\n",
      "Epoch [20/500], Train Loss: 0.0000, Train Accuracy: 1.0000\n",
      "Epoch [20/500], Validation Loss: 0.1245, Validation Accuracy: 0.8690\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f39cba9f934ff1acb891f791a93c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.0803, Train Accuracy: 0.6631\n",
      "Epoch [1/500], Validation Loss: 0.0699, Validation Accuracy: 0.7024\n",
      "Epoch [2/500], Train Loss: 0.0469, Train Accuracy: 0.8242\n",
      "Epoch [2/500], Validation Loss: 0.0634, Validation Accuracy: 0.7976\n",
      "Epoch [3/500], Train Loss: 0.0407, Train Accuracy: 0.8642\n",
      "Epoch [3/500], Validation Loss: 0.0708, Validation Accuracy: 0.7619\n",
      "Epoch [4/500], Train Loss: 0.0292, Train Accuracy: 0.9148\n",
      "Epoch [4/500], Validation Loss: 0.0508, Validation Accuracy: 0.8214\n",
      "Epoch [5/500], Train Loss: 0.0179, Train Accuracy: 0.9481\n",
      "Epoch [5/500], Validation Loss: 0.0452, Validation Accuracy: 0.8690\n",
      "Epoch [6/500], Train Loss: 0.0134, Train Accuracy: 0.9561\n",
      "Epoch [6/500], Validation Loss: 0.0588, Validation Accuracy: 0.8095\n",
      "Epoch [7/500], Train Loss: 0.0110, Train Accuracy: 0.9640\n",
      "Epoch [7/500], Validation Loss: 0.0526, Validation Accuracy: 0.8571\n",
      "Epoch [8/500], Train Loss: 0.0048, Train Accuracy: 0.9867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m one_hot_targets \u001b[39m=\u001b[39m one_hot_targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     71\u001b[0m \u001b[39m# Zero the gradients\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_inputs)\n\u001b[1;32m     75\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     76\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, one_hot_targets\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m, in \u001b[0;36mCNNTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     48\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     50\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     x\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/miniconda3/envs/mtp/lib/python3.10/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_lst = []\n",
    "for idx, config in enumerate(configs):\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_val_loss = np.inf\n",
    "    writer = SummaryWriter(log_dir = f'{tensorboard_path}/transformer_{idx}')\n",
    "    dim_feedforward, nhead, learning_rate, weight_decay = config\n",
    "    #dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "\n",
    "    # Create an instance of the GRU classifier model\n",
    "    model = CNNTransformer(input_dim, output_dim, d_model, nhead, dim_feedforward, dropout, activation=\"relu\").to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    patience = 20\n",
    "    patience_check = 0\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        #train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        for batch_inputs, batch_targets in train_loader:\n",
    "            # Move data to the device\n",
    "            batch_inputs = batch_inputs.squeeze().to(device)\n",
    "            one_hot_targets = F.one_hot(batch_targets.long(), num_classes=3)\n",
    "            one_hot_targets = one_hot_targets.to(device)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_inputs)\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, one_hot_targets.float())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss and count correct predictions\n",
    "            train_loss += loss.item()\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            train_acc += (predicted == batch_targets.to(device)).sum().item()\n",
    "\n",
    "        # Print the average loss and accuracy for the epoch\n",
    "        # Print loss and accuracy for epoch\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc /= len(train_loader.dataset)\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", train_acc, epoch)   \n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f}'.format(epoch+1, num_epochs, train_loss, train_acc))\n",
    "\n",
    "        # validation\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        # Initialize variables to track loss and accuracy\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        # Disable gradient computation\n",
    "        with torch.no_grad():\n",
    "            # Iterate over validation data\n",
    "            for batch_inputs, batch_targets in val_loader:\n",
    "                batch_inputs = batch_inputs.squeeze().to(device)\n",
    "                one_hot_targets = F.one_hot(batch_targets.long(), num_classes=3)\n",
    "                one_hot_targets = one_hot_targets.to(device)\n",
    "                # Zero the gradients\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_inputs)\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, one_hot_targets.float())\n",
    "                val_loss += loss.item()\n",
    "                predicted = torch.argmax(outputs, 1)\n",
    "                val_acc += (predicted == batch_targets.to(device)).sum().item()\n",
    "                # Backward pass and optimization\n",
    "            \n",
    "            # Compute average loss and accuracy for epoch\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_acc /= len(val_loader.dataset)\n",
    "            writer.add_scalar(\"Loss/validation\", val_loss, epoch)\n",
    "            writer.add_scalar(\"Acc/validation\", val_acc, epoch)\n",
    "            print('Epoch [{}/{}], Validation Loss: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, num_epochs, val_loss, val_acc))\n",
    "            \n",
    "            # early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                patience_check = 0\n",
    "            else:\n",
    "                patience_check += 1 \n",
    "\n",
    "            if patience_check >= patience:\n",
    "                break\n",
    "    # Save the trained model\n",
    "    # torch.save(model.state_dict(), f\"./transformer/transformer_classifier.pth\")\n",
    "    loss_lst.append(val_loss)\n",
    "    torch.save({\n",
    "                'input_dim': input_dim,\n",
    "                'output_dim': output_dim,\n",
    "                'd_model': d_model,\n",
    "                'nhead': nhead,\n",
    "                'dim_feedforward': dim_feedforward,\n",
    "                'dropout': dropout,\n",
    "                'model_state_dict': best_model.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                }, \"/ceph/inestp02/stroke_classifier/src/transformer/transformer_classifier_{idx}.pth\")\n",
    "    torch.cuda.empty_cache()\n",
    "with open(\"/ceph/inestp02/stroke_classifier/src/transformer/loss_lst.pickle\", \"wb\") as f:\n",
    "    pickle.dump(loss_lst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test_accuracy(model):\n",
    "    #test\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle = False)\n",
    "\n",
    "    # Initialize variables to track loss and accuracy\n",
    "    # test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over validation data\n",
    "        for batch_inputs, batch_targets in test_loader:\n",
    "            batch_inputs = batch_inputs.squeeze().to(device)\n",
    "            one_hot_targets = F.one_hot(batch_targets.long(), num_classes=3)\n",
    "            one_hot_targets = one_hot_targets.to(device)\n",
    "            \n",
    "            outputs = model(batch_inputs)\n",
    "            # Compute the loss\n",
    "            # loss = criterion(outputs, one_hot_targets.float())\n",
    "\n",
    "            # # Update loss and accuracy\n",
    "            # test_loss += loss.item()\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            test_acc += (predicted == batch_targets.to(device)).sum().item()\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "    test_acc /= len(test_loader.dataset)\n",
    "    \n",
    "    # print(\"Classification report\")\n",
    "    # print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    # Calculate and print confusion matrix\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    print(\"f1 score:\", f1_score(true_labels, predicted_labels, average=None))\n",
    "    cf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(cf_matrix)\n",
    "    sns.heatmap(cf_matrix, annot=True)\n",
    "    \n",
    "    # Compute average loss and accuracy for epoch\n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # writer.add_scalar(\"Loss/test\", test_loss)\n",
    "    # writer.add_scalar(\"Acc/test\", test_acc)     \n",
    "    # Print loss and accuracy for epoch\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/ceph/inestp02/stroke_classifier/src/transformer/loss_lst.pickle\", 'rb') as f:\n",
    "    loss_lst = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230, 215, 122, 203,  57,  11,  81, 214,  33,  69])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_np = np.array(loss_lst)\n",
    "sorted_idx = np.argsort(loss_np)[:10]\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc of 230th model: 0.7353760445682451\n",
      "test acc of 215th model: 0.7910863509749304\n",
      "test acc of 122th model: 0.7576601671309192\n",
      "test acc of 203th model: 0.7325905292479109\n",
      "test acc of 57th model: 0.7075208913649025\n",
      "test acc of 11th model: 0.6239554317548747\n",
      "test acc of 81th model: 0.83008356545961\n",
      "test acc of 214th model: 0.7883008356545961\n",
      "test acc of 33th model: 0.8328690807799443\n",
      "test acc of 69th model: 0.8022284122562674\n"
     ]
    }
   ],
   "source": [
    "for idx in sorted_idx:\n",
    "    checkpoint = torch.load(f\"./transformer/transformer_classifier_{idx}.pth\")\n",
    "\n",
    "    input_dim = checkpoint['input_dim']\n",
    "    output_dim = checkpoint['output_dim']\n",
    "    d_model = checkpoint['d_model']\n",
    "    nhead = checkpoint['nhead']\n",
    "    dim_feedforward = checkpoint['dim_feedforward']\n",
    "    dropout = checkpoint['dropout']\n",
    "\n",
    "    model = CNNTransformer(input_dim, output_dim, d_model, nhead, dim_feedforward, dropout, activation=\"relu\").to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"test acc of {idx}th model:\", test_accuracy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: [0.75313808 0.75303644 0.99137931]\n",
      "[[ 90  27   0]\n",
      " [ 32  93   2]\n",
      " [  0   0 115]]\n",
      "test acc of 69th model: 0.83008356545961\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load(f\"./transformer/transformer_classifier_81.pth\")\n",
    "\n",
    "input_dim = checkpoint['input_dim']\n",
    "output_dim = checkpoint['output_dim']\n",
    "d_model = checkpoint['d_model']\n",
    "nhead = checkpoint['nhead']\n",
    "dim_feedforward = checkpoint['dim_feedforward']\n",
    "dropout = checkpoint['dropout']\n",
    "\n",
    "model = CNNTransformer(input_dim, output_dim, d_model, nhead, dim_feedforward, dropout, activation=\"relu\").to(device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"test acc of {idx}th model:\", test_accuracy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
